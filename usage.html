

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>How to use DrEvalPy &mdash; DrEvalPy 1.4.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/custom_cookietemple.css?v=a7e27724" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=350a9c04"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Run your own model" href="runyourmodel.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html">
            
              <img src="_static/drevalpy.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to use DrEvalPy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#run-a-drug-response-experiment-results-with-drevalpy">Run a drug response experiment results with <code class="docutils literal notranslate"><span class="pre">drevalpy</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#visualize-and-evaluate-results-with-drevalpy-report">Visualize and evaluate results with <code class="docutils literal notranslate"><span class="pre">drevalpy-report</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#available-settings">Available Settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#available-models">Available Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#available-datasets">Available Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#corresponding-feature-data">Corresponding feature data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#custom-datasets">Custom Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#available-randomization-tests">Available Randomization Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robustness-test">Robustness Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="#available-metrics">Available Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#available-response-transformations">Available Response Transformations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="runyourmodel.html">Run your own model</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">How to cite</a></li>
<li class="toctree-l1"><a class="reference internal" href="memes.html">Memes and fun stuff</a></li>
<li class="toctree-l1"><a class="reference internal" href="API.html">API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DrEvalPy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">How to use DrEvalPy</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/daisybio/drevalpy/blob/development/docs/usage.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="how-to-use-drevalpy">
<h1>How to use DrEvalPy<a class="headerlink" href="#how-to-use-drevalpy" title="Link to this heading"></a></h1>
<p>Here, we document how to run DrEval with our implemented models and datasets. You can either do this with the standalone
supplied here or with the associated Nextflow pipeline <code class="docutils literal notranslate"><span class="pre">drugresponseeval</span></code>. We recommend the use of our Nextflow pipeline for computational
demanding runs and for improved reproducibility.
No knowledge of Nextflow is required to run it. The Nextflow pipeline is available on the <a class="reference external" href="https://github.com/nf-core/drugresponseeval.git">nf-core GitHub</a>, the corresponding documentation can be found
<a class="reference external" href="https://nf-co.re/drugresponseeval/dev/">here</a>. Documentation of the standalone is provided below.</p>
<section id="run-a-drug-response-experiment-results-with-drevalpy">
<h2>Run a drug response experiment results with <code class="docutils literal notranslate"><span class="pre">drevalpy</span></code><a class="headerlink" href="#run-a-drug-response-experiment-results-with-drevalpy" title="Link to this heading"></a></h2>
<p>You can run it the drug response pipeline, which can test drug response models via:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>drevalpy<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span><span class="w"> </span><span class="o">[</span>--run_id<span class="w"> </span>RUN_ID<span class="o">]</span><span class="w"> </span><span class="o">[</span>--path_data<span class="w"> </span>PATH_DATA<span class="o">]</span><span class="w"> </span><span class="o">[</span>--models<span class="w"> </span>MODELS<span class="w"> </span><span class="o">[</span>MODELS<span class="w"> </span>...<span class="o">]]</span><span class="w"> </span><span class="o">[</span>--baselines<span class="w"> </span>BASELINES<span class="w"> </span><span class="o">[</span>BASELINES<span class="w"> </span>...<span class="o">]]</span><span class="w"> </span><span class="o">[</span>--test_mode<span class="w"> </span>TEST_MODE<span class="w"> </span><span class="o">[</span>TEST_MODE<span class="w"> </span>...<span class="o">]]</span>
<span class="w">                </span><span class="o">[</span>--randomization_mode<span class="w"> </span>RANDOMIZATION_MODE<span class="w"> </span><span class="o">[</span>RANDOMIZATION_MODE<span class="w"> </span>...<span class="o">]]</span><span class="w"> </span><span class="o">[</span>--randomization_type<span class="w"> </span>RANDOMIZATION_TYPE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--n_trials_robustness<span class="w"> </span>N_TRIALS_ROBUSTNESS<span class="o">]</span><span class="w"> </span><span class="o">[</span>--dataset_name<span class="w"> </span>DATASET_NAME<span class="o">]</span>
<span class="w">                </span><span class="o">[</span>--cross_study_datasets<span class="w"> </span>CROSS_STUDY_DATASETS<span class="w"> </span><span class="o">[</span>CROSS_STUDY_DATASETS<span class="w"> </span>...<span class="o">]]</span><span class="w"> </span><span class="o">[</span>--path_out<span class="w"> </span>PATH_OUT<span class="o">]</span><span class="w"> </span><span class="o">[</span>--measure<span class="w"> </span>MEASURE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--no_refitting<span class="o">]</span><span class="w"> </span><span class="o">[</span>--curve_curator_cores<span class="w"> </span>CORES<span class="o">]</span><span class="w"> </span><span class="o">[</span>--overwrite<span class="o">]</span><span class="w"> </span><span class="o">[</span>--optim_metric<span class="w"> </span>OPTIM_METRIC<span class="o">]</span><span class="w"> </span><span class="o">[</span>--n_cv_splits<span class="w"> </span>N_CV_SPLITS<span class="o">]</span>
<span class="w">                </span><span class="o">[</span>--response_transformation<span class="w"> </span>RESPONSE_TRANSFORMATION<span class="o">]</span><span class="w"> </span><span class="o">[</span>--multiprocessing<span class="o">]</span><span class="w"> </span><span class="o">[</span>--model_checkpoint_dir<span class="w"> </span>MODEL_CHECKPOINT_DIR<span class="o">]</span><span class="w"> </span><span class="o">[</span>--final_model_on_full_data<span class="o">]</span><span class="w"> </span><span class="o">[</span>--no_hyperparameter_tuning<span class="o">]</span>
</pre></div>
</div>
<p>Options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-h,</span> <span class="pre">--help</span></code>: Show help message and exit.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--run_id</span> <span class="pre">RUN_ID</span></code>: Identifier for the run. Will be used as a prefix for all output files.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--path_data</span> <span class="pre">PATH_DATA</span></code>: Path to the data directory, default: data. All data files should be stored in this directory and will be downloaded into this directory. The location of the datasets are resolved by <code class="docutils literal notranslate"><span class="pre">&lt;path_data&gt;/&lt;dataset_name&gt;/&lt;dataset_name&gt;.csv</span></code>. If providing raw viability data, the file needs to be named <code class="docutils literal notranslate"><span class="pre">&lt;dataset_name&gt;_raw.csv</span></code> instead and <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> needs to be unspecified for automated curve fitting (thats the default) (see <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> for details and also check the <a class="reference internal" href="#custom-datasets"><span class="std std-ref">Custom Datasets</span></a> section).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--models</span> <span class="pre">MODELS</span> <span class="pre">[MODELS</span> <span class="pre">...]</span></code>: List of models to evaluate. For a list of available models, see the <a class="reference internal" href="#available-models"><span class="std std-ref">Available Models</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--baselines</span> <span class="pre">BASELINES</span> <span class="pre">[BASELINES</span> <span class="pre">...]</span></code>: List of baselines to evaluate. If NaiveMeanEffectsPredictor is not part of them, we will add it. For a list of available baselines, see the <a class="reference internal" href="#available-models"><span class="std std-ref">Available Models</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--test_mode</span> <span class="pre">TEST_MODE</span> <span class="pre">[TEST_MODE</span> <span class="pre">...]</span></code>: Which tests to run (LPO=Leave-random-Pairs-Out, LCO=Leave-Cell-line-Out, LTO=Leave-Tissue-Out, LDO=Leave-Drug-Out). Can be a list of test runs e.g. ‘LPO LCO LTO LDO’ to run all tests. Default is LPO. For more information, see the <a class="reference internal" href="#available-settings"><span class="std std-ref">Available Settings</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--randomization_mode</span> <span class="pre">RANDOMIZATION_MODE</span> <span class="pre">[RANDOMIZATION_MODE</span> <span class="pre">...]</span></code>: Which randomization mode to use. Can be a list of randomization modes e.g. ‘SVCC SVCD SVRC SVRD’ to run all randomization modes. Default is None. For more information, see the <a class="reference internal" href="#available-randomization-tests"><span class="std std-ref">Available Randomization Tests</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--randomization_type</span> <span class="pre">RANDOMIZATION_TYPE</span></code>: Which randomization type to use. Default is ‘permutation’. For more information, see the <a class="reference internal" href="#available-randomization-tests"><span class="std std-ref">Available Randomization Tests</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--n_trials_robustness</span> <span class="pre">N_TRIALS_ROBUSTNESS</span></code>: Number of trials for robustness testing. Default is 0, which means no robustness testing. For more information, see the <a class="reference internal" href="#robustness-test"><span class="std std-ref">Robustness Test</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--dataset_name</span> <span class="pre">DATASET_NAME</span></code>: Name of the dataset to use. For a list of available datasets, see the <a class="reference internal" href="#available-datasets"><span class="std std-ref">Available Datasets</span></a> section. For information on how to use custom datasets, see the <a class="reference internal" href="#custom-datasets"><span class="std std-ref">Custom Datasets</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--cross_study_datasets</span> <span class="pre">CROSS_STUDY_DATASETS</span> <span class="pre">[CROSS_STUDY_DATASETS</span> <span class="pre">...]</span></code>: List of datasets to use for cross-study validation. For a list of available datasets, see the <a class="reference internal" href="#available-datasets"><span class="std std-ref">Available Datasets</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--path_out</span> <span class="pre">PATH_OUT</span></code>: Path to the output directory, default: results. All output files will be stored in this directory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--measure</span> <span class="pre">MEASURE</span></code>: The name of the measure to use, default ‘LN_IC50’. If using one of the available datasets (see <code class="docutils literal notranslate"><span class="pre">--dataset_name</span></code>), this is restricted to one of [‘LN_IC50’, ‘EC50’, ‘IC50’, ‘pEC50’, ‘AUC’, ‘response’]. This corresponds to the names of the columns that contain theses measures in the provided input dataset. If providing a custom dataset, this may differ. If the option <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> is not set, the prefix ‘_curvecurator’ is automatically appended, e.g. ‘LN_IC50_curvecurator’, to allow using the refit measures instead of the ones originally published for the available datasets, allowing for better dataset comparability (refit measures are already provided in the available datasets or computed as part of the fitting procedure when providing custom raw viability datasets, see <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> for details).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code>: If not set, the measure is appended with ‘_curvecurator’. If a custom dataset_name was provided, this will invoke the fitting procedure of raw viability data, which is expected to exist at <code class="docutils literal notranslate"><span class="pre">&lt;path_data&gt;/&lt;dataset_name&gt;/&lt;dataset_name&gt;_raw.csv</span></code>. The fitted dataset will be stored in the same folder, in a file called <code class="docutils literal notranslate"><span class="pre">&lt;dataset_name&gt;.csv</span></code>. Also check the <a class="reference internal" href="#custom-datasets"><span class="std std-ref">Custom Datasets</span></a> section. Default is False i.e. curvecurated drug response measures are utilzed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--curve_curator_cores</span> <span class="pre">CURVE_CURATOR_CORES</span></code>: Number of cores to use for CurveCurator fitting. Only used when <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> is not set. Default is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--overwrite</span></code>: If set, existing files will be overwritten.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--optim_metric</span> <span class="pre">OPTIM_METRIC</span></code>: The metric to optimize for during hyperparameter tuning. Default is ‘RMSE’. For more information, see the <a class="reference internal" href="#available-metrics"><span class="std std-ref">Available Metrics</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--n_cv_splits</span> <span class="pre">N_CV_SPLITS</span></code>: Number of cross-validation splits. Default is 7.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--response_transformation</span> <span class="pre">RESPONSE_TRANSFORMATION</span></code>: Transformation to apply to the response data. Default is None. For more information, see the <a class="reference internal" href="#available-response-transformations"><span class="std std-ref">Available Response Transformations</span></a> section.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--multiprocessing</span></code>: If set, we will use raytune for fitting. Default is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--model_checkpoint_dir</span> <span class="pre">MODEL_CHECKPOINT_DIR</span></code>: Directory to save model checkpoints. Default is ‘TEMPORARY’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--final_model_on_full_data</span></code>: If set, saves a final model trained/tuned on the union of all folds after CV. Default is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--no_hyperparameter_tuning</span></code>: If set, disables hyperparameter tuning and uses the first hyperparameter set. Default is False.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>drevalpy<span class="w"> </span>--run_id<span class="w"> </span>my_first_run<span class="w"> </span>--models<span class="w"> </span>NaiveDrugMeanPredictor<span class="w"> </span>ElasticNet<span class="w"> </span>--dataset<span class="w"> </span>TOYv1<span class="w"> </span>--test_mode<span class="w"> </span>LCO
</pre></div>
</div>
<p><em>Note</em>: You need at least 7 CV splits to get a meaningful critical difference diagram and the corresponding p-values.</p>
</section>
<section id="visualize-and-evaluate-results-with-drevalpy-report">
<h2>Visualize and evaluate results with <code class="docutils literal notranslate"><span class="pre">drevalpy-report</span></code><a class="headerlink" href="#visualize-and-evaluate-results-with-drevalpy-report" title="Link to this heading"></a></h2>
<p>Executing the main script <code class="docutils literal notranslate"><span class="pre">drevalpy</span></code> will generate a folder with the results which includes the predictions of all models
in all specified settings. The <code class="docutils literal notranslate"><span class="pre">drevalpy-report</span></code> CLI will evaluate the results with all available metrics and create an
HTML report with many visualizations. You can run it with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>drevalpy-report<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span><span class="w"> </span>--run_id<span class="w"> </span>RUN_ID<span class="w"> </span>--dataset<span class="w"> </span>DATASET<span class="w"> </span><span class="o">[</span>--path_data<span class="w"> </span>PATH_DATA<span class="o">]</span><span class="w"> </span><span class="o">[</span>--result_path<span class="w"> </span>RESULT_PATH<span class="o">]</span>
</pre></div>
</div>
<p>Options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-h,</span> <span class="pre">--help</span></code>: Show help message and exit.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--run_id</span> <span class="pre">RUN_ID</span></code>: Identifier for the run which was used when executing the <code class="docutils literal notranslate"><span class="pre">drevalpy</span></code> command.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--dataset</span> <span class="pre">DATASET</span></code>: Name of the dataset which was used when executing the <code class="docutils literal notranslate"><span class="pre">drevalpy</span></code> command.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--path_data</span> <span class="pre">PATH_DATA</span></code>: Path to the data directory, default: data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--result_path</span> <span class="pre">RESULT_PATH</span></code>: Path to the results directory, default: results.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>drevalpy-report<span class="w"> </span>--run_id<span class="w"> </span>my_first_run<span class="w"> </span>--dataset<span class="w"> </span>TOYv1
</pre></div>
</div>
<p>The report will be stored in the <code class="docutils literal notranslate"><span class="pre">results/RUN_ID</span></code> folder.
You can open the <code class="docutils literal notranslate"><span class="pre">index.html</span></code> file in your browser to view the report.</p>
</section>
<section id="available-settings">
<h2>Available Settings<a class="headerlink" href="#available-settings" title="Link to this heading"></a></h2>
<p>DrEval is designed to ensure that drug response prediction models are evaluated in a consistent and
reproducible manner. We offer three settings via the <code class="docutils literal notranslate"><span class="pre">--test_mode</span></code> parameter:</p>
<a class="reference internal image-reference" href="_images/LPO.png"><img alt="Image visualizing the Leave-Pair-Out setting" src="_images/LPO.png" style="width: 24%;" />
</a>
<a class="reference internal image-reference" href="_images/LCO.png"><img alt="Image visualizing the Leave-Cell-Line-Out setting" src="_images/LCO.png" style="width: 24%;" />
</a>
<a class="reference internal image-reference" href="_images/LTO.png"><img alt="Image visualizing the Leave-Tissue-Out setting" src="_images/LTO.png" style="width: 24%;" />
</a>
<a class="reference internal image-reference" href="_images/LDO.png"><img alt="Image visualizing the Leave-Drug-Out setting" src="_images/LDO.png" style="width: 24%;" />
</a>
<ul class="simple">
<li><p><strong>Leave-Pair-Out (LPO)</strong>: Random pairs of cell lines and drugs are left out for validation/testing but both the drug and the
cell line might already be present in the training set. This is the <strong>easiest setting</strong> for your model but also the
most uninformative one. The only application scenario for this setting is when you want to test whether your model
can <strong>complete the missing values in the training set</strong>.</p></li>
<li><p><strong>Leave-Cell-Line-Out (LCO)</strong>: Random cell lines are left out for validation/testing but the drugs might already be present in
the training set. This setting is <strong>more challenging</strong> than LPO but still relatively easy. The application scenario
for this setting is when you want to test whether your model can <strong>predict the response of a new cell line</strong>. This
is very relevant for <strong>personalized medicine or drug repurposing</strong>.</p></li>
<li><p><strong>Leave-Drug-Out (LDO)</strong>: Random drugs are left out for validation/testing but the cell lines might already be present in the
training set. This setting is the <strong>most challenging</strong> one. The application scenario for this setting is when you
want to test whether your model can <strong>predict the response of a new drug</strong>. This is very relevant for <strong>drug
development</strong>.</p></li>
</ul>
<p>An underlying issue is that drugs have a rather unique IC50 range. That means that by just predicting the mean IC50
that a drug has in the training set (aggregated over all cell lines), you can already achieve a seemingly good
prediction (as evaluated by naive R^2 or correlation metrics). This is why we also offer the possibility to compare your model to a <strong>NaivePredictor</strong> that predicts
the mean IC50 of all drugs in the training set. We also offer several less naive predictors:
<strong>NaiveCellLineMeanPredictor</strong>, <strong>NaiveDrugMeanPredictor</strong>, <strong>NaiveTissueMeanPredictor</strong>, and <strong>NaiveTissueDrugMeanPredictor</strong>.
The <strong>NaiveCellLineMeanPredictor</strong> predicts the mean IC50 of a cell line in the training set,
the <strong>NaiveDrugMeanPredictor</strong> predicts the mean IC50 of a drug in the training set,
the <strong>NaiveTissueMeanPredictor</strong> predicts the mean IC50 of a tissue in the training set,
and the <strong>NaiveTissueDrugMeanPredictor</strong> predicts the mean IC50 per tissue-drug combination (aggregated across all cell lines with that tissue-drug pair).
The <strong>NaiveMeanEffectPredictor</strong> combines the effects of cell lines and drugs.
It is equivalent to the <strong>NaiveCellLineMeanPredictor</strong> and <strong>NaiveDrugMeanPredictor</strong> for the LDO and LCO settings, respectively,
as test cell line effects and drug effects are unknown in these settings.</p>
<p>In LCO, <strong>NaiveTissueDrugMeanPredictor</strong> is the strongest baseline, while in all other settings, <strong>NaiveMeanEffectPredictor</strong> is the strongest.</p>
</section>
<section id="available-models">
<h2>Available Models<a class="headerlink" href="#available-models" title="Link to this heading"></a></h2>
<p>In addition to the Naive Predictors, we offer a variety of more advanced <strong>baseline models</strong> and
some <strong>state-of-the-art models</strong> to compare your model against. You can either set them as baselines or as models via the
<code class="docutils literal notranslate"><span class="pre">--models</span></code> and <code class="docutils literal notranslate"><span class="pre">--baselines</span></code> parameters.
We first identify the best hyperparameters for all models and baselines in a cross-validation setting. Then, we
train the models on the whole training set and evaluate them on the test set.
For <code class="docutils literal notranslate"><span class="pre">--models</span></code>, you can also perform randomization and robustness tests. The <code class="docutils literal notranslate"><span class="pre">--baselines</span></code> are skipped for these tests.</p>
</section>
<section id="available-datasets">
<h2>Available Datasets<a class="headerlink" href="#available-datasets" title="Link to this heading"></a></h2>
<p>We provide commonly used datasets to evaluate your model on (GDSC1, GDSC2, CCLE, CTRPv2) via the <code class="docutils literal notranslate"><span class="pre">--dataset_name</span></code> parameter.
Further, we provide 2 datasets with more clinical relevance: BeatAML2 and PDX_Bruna.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Dataset Name</p></th>
<th class="head"><p>Number of DRP Curves</p></th>
<th class="head"><p>Number of Drugs</p></th>
<th class="head"><p>Number of Cell Lines</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GDSC1</p></td>
<td><p>316,506</p></td>
<td><p>378</p></td>
<td><p>970</p></td>
<td><p>The Genomics of Drug Sensitivity in Cancer (GDSC) dataset version 1.</p></td>
</tr>
<tr class="row-odd"><td><p>GDSC2</p></td>
<td><p>234,437</p></td>
<td><p>287</p></td>
<td><p>969</p></td>
<td><p>The Genomics of Drug Sensitivity in Cancer (GDSC) dataset version 2.</p></td>
</tr>
<tr class="row-even"><td><p>CCLE</p></td>
<td><p>11,670</p></td>
<td><p>24</p></td>
<td><p>503</p></td>
<td><p>The Cancer Cell Line Encyclopedia (CCLE) dataset.</p></td>
</tr>
<tr class="row-odd"><td><p>CTRPv1</p></td>
<td><p>60,758</p></td>
<td><p>354</p></td>
<td><p>243</p></td>
<td><p>The Cancer Therapeutics Response Portal (CTRP) dataset version 1.</p></td>
</tr>
<tr class="row-even"><td><p>CTRPv2</p></td>
<td><p>395,025</p></td>
<td><p>546</p></td>
<td><p>886</p></td>
<td><p>The Cancer Therapeutics Response Portal (CTRP) dataset version 2.</p></td>
</tr>
<tr class="row-odd"><td><p>TOYv1</p></td>
<td><p>2,711</p></td>
<td><p>36</p></td>
<td><p>90</p></td>
<td><p>A toy dataset for testing purposes subsetted from CTRPv2.</p></td>
</tr>
<tr class="row-even"><td><p>TOYv2</p></td>
<td><p>2,784</p></td>
<td><p>36</p></td>
<td><p>90</p></td>
<td><p>A second toy dataset for cross study testing purposes. 80 cell lines and 32 drugs overlap TOYv2.</p></td>
</tr>
<tr class="row-odd"><td><p>BeatAML2</p></td>
<td><p>62,487</p></td>
<td><p>166</p></td>
<td><p>569 (patients)</p></td>
<td><p>Ex vivo drug sensitivity screening for a cohort of acute myeloid leukemia (AML) patients.</p></td>
</tr>
<tr class="row-even"><td><p>PDX_Bruna</p></td>
<td><p>2,559</p></td>
<td><p>104</p></td>
<td><p>37 (mouse passages)</p></td>
<td><p>Ex vivo drug sensitivity screening for short-term cultures of PDTX-derived tumor cells from breast cancer patients</p></td>
</tr>
</tbody>
</table>
<p>If not specifying <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> option with these datasets (default: false), the desired measure provided with the <code class="docutils literal notranslate"><span class="pre">--measure</span></code> option is appended with “_curvecurator”, e.g. “IC50_curvecurator”.
In the provided datasets, these are the measures calculated with the same fitting procedure using CurveCurator. To use the measures reported from the original publications of the
dataset, use the <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> option, which will use the original measures as provided in the datasets.</p>
<p>This however makes it hard to do cross-study comparisons, since the measures may not be directly comparable due to differences in the fitting procedures used by the original authors.
It is therefore recommended to always use DrEvalPy without the <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> option, which will lead to the use of the refitted measures that are calculated with the same procedure for all datasets.</p>
</section>
<section id="corresponding-feature-data">
<h2>Corresponding feature data<a class="headerlink" href="#corresponding-feature-data" title="Link to this heading"></a></h2>
<p>The datasets have corresponding cell-line and drug feature data. The sources are as follows:</p>
<ul class="simple">
<li><dl class="simple">
<dt>GDSC1 &amp; 2:</dt><dd><ul>
<li><p>Gene expression: RMA-normalized microarray expression data from the <a class="reference external" href="https://www.cancerrxgene.org/downloads/bulk_download">GDSC Data Portal</a> (raw data).</p></li>
<li><p>Methylation: Preprocessed Beta Values for all CpG islands, IlluminaHumanMethylation450 BeadChip <a class="reference external" href="https://www.cancerrxgene.org/gdsc1000/GDSC1000_WebResources/Home.html">GDSC Data Portal</a>.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>CCLE, CTRPv1, CTRPv2:</dt><dd><ul>
<li><p>Gene expression: reprocessed RNA-seq data PRJNA523380</p></li>
<li><p>Methylation: DepMap Beta Values for RRBS clusters <code class="docutils literal notranslate"><span class="pre">CCLE_RRBS_TSS_CpG_clusters_20180614.txt</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Used by GDSC1, 2, CCLE, CTRPv1 and v2:</dt><dd><ul>
<li><p>Mutation &amp; CNV data: <a class="reference external" href="https://cellmodelpassports.sanger.ac.uk/downloads">Sanger Cell Model Passports</a>.</p></li>
<li><p>Proteomics: Raw data at PRIDE: PXD030304</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>BeatAML2:</dt><dd><ul>
<li><p>Gene expression: RNA-seq but not re-processed because of missing FASTQ files. Taken from <a class="reference external" href="https://biodev.github.io/BeatAML2/">the corresponding website</a></p></li>
<li><p>Mutation data would have been available but is measured too shallow, so we chose not to include it</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PDX_Bruna:</dt><dd><ul>
<li><p>Retrieved from <a class="reference external" href="https://figshare.com/s/4a3f6bc543e5ba85834c">the corresponding figshare</a></p></li>
<li><p>Gene expression: Microarray expression data</p></li>
<li><p>Copy number variation: Reprocessed with GISTIC2.0</p></li>
<li><p>Mutation data would have been available but is measured too shallow, so we chose not to include it</p></li>
<li><p>Methylation data would have been available but only Promoter methylation data which is incompatible with the CpG methylation data we have for the other screens.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Drug features</dt><dd><ul>
<li><p>Morgan Fingerprints were generated with RDKit from SMILES either downloaded from PubChem or provided by GDSC.</p></li>
<li><dl class="simple">
<dt><a class="reference external" href="https://drive.google.com/drive/folders/16hP48-noHi3-c_LP9TcZxkwAzqxgR0VB">DIPK associated drive</a></dt><dd><ul>
<li><p>MolGNet features were generated from SMILES</p></li>
<li><p>BIONIC features were generated from top expressed genes</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Gene lists</dt><dd><ul>
<li><p>The 978 landmark genes are from the L1000 assay</p></li>
<li><p>The drug target genes are the genes targeted by the drugs used in GDSC, extractable from the <a class="reference external" href="https://www.cancerrxgene.org/downloads/bulk_download">GDSC Data Portal</a> (compounds annotation).</p></li>
<li><p>The intersection lists are features occurring in all datasets for the respective OMICs to ensure that cross-study predictions can easily be done because the features are shared.</p></li>
<li><p>Reduced versions of the lists only containing genes occurring in all datasets</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>For more information on the preprocessing, please refer to <a class="reference external" href="https://github.com/daisybio/preprocess_drp_data">the corresponding GitHub Repo</a>.</p>
</section>
<section id="custom-datasets">
<h2>Custom Datasets<a class="headerlink" href="#custom-datasets" title="Link to this heading"></a></h2>
<p>You can also provide your own custom dataset via the <code class="docutils literal notranslate"><span class="pre">--dataset_name</span></code> parameter by specifying a name that is not in the list of the available datasets.
This can be prefit data (not recommended for comparability reasons) or raw viability data that is automatically fit with the exact same procedure that was used to refit
the available datasets in the previous section.</p>
<p><strong>Raw viability data</strong></p>
<ul class="simple">
<li><p>DrEvalPy expects a csv-formatted file in the location <code class="docutils literal notranslate"><span class="pre">&lt;path_data&gt;/&lt;dataset&gt;/&lt;dataset_name&gt;_raw.csv</span></code> (corresponding to the <code class="docutils literal notranslate"><span class="pre">--path_data</span></code> and <code class="docutils literal notranslate"><span class="pre">--dataset_name</span></code> options), which contains the raw viability data in long format with the columns [“dose”, “response”, “sample”, “drug”] and an optional “replicate” column. If replicates are provided, the procedure will fit one curve per sample / drug pair using all replicates.</p></li>
<li><dl class="simple">
<dt><strong>All dosages have to be provided in µM!</strong> Drevalpy will compute the following response measures:</dt><dd><ul>
<li><p>pEC50_curvecurator: computed internally by CurveCurator. Is computed as -log10(EC50_curvecurator[M]).</p></li>
<li><p>EC50_curvecurator: given in µM</p></li>
<li><p>IC50_curvecurator: given in µM</p></li>
<li><p>LN_IC50_curvecurator: computed from IC50_curvecurator</p></li>
<li><p>AUC_curvecurator</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>The option <code class="docutils literal notranslate"><span class="pre">--curve_curator_cores</span></code> must be set. <code class="docutils literal notranslate"><span class="pre">--no_refitting</span></code> must not be set.</p></li>
<li><p>DrEvalPy provides all results of the fitting in the same folder including the fitted curves in a file folder <code class="docutils literal notranslate"><span class="pre">&lt;path_data&gt;/&lt;dataset&gt;/&lt;dataset_name&gt;.csv</span></code></p></li>
</ul>
<p><strong>Prefit viability data</strong></p>
<ul class="simple">
<li><p>DrEvalPy expects a csv-formatted file in the location <code class="docutils literal notranslate"><span class="pre">&lt;path_data&gt;/&lt;dataset&gt;/&lt;dataset_name&gt;.csv</span></code> (corresponding to the <code class="docutils literal notranslate"><span class="pre">--path_data</span></code> and <code class="docutils literal notranslate"><span class="pre">--dataset_name</span></code> options),
with at least the columns [“cell_line_id”, “drug_id”, &lt;measure&gt;”] where &lt;measure&gt; is replaced with the name of the measure you provide.</p></li>
<li><p>For LTO, you must also provide a “tissue” column with tissue information</p></li>
<li><p>Available measures depend on the column names and can be provided using the <cite>–measure</cite> option.</p></li>
<li><p>It is required that you use measure names that are also working with the available datasets if you use the <code class="docutils literal notranslate"><span class="pre">--cross_study_datasets</span></code> option</p></li>
<li><p>Your dataset will be read in with the DrugResponseDataset.from_csv method (drevalpy.datasets.dataset); <a class="reference download internal" download="" href="_downloads/5279763bf4143d4a4919d03efa9445cb/response_example.csv"><code class="xref download docutils literal notranslate"><span class="pre">Example</span> <span class="pre">response</span> <span class="pre">file</span></code></a> would support the measure AUC.</p></li>
</ul>
</section>
<section id="available-randomization-tests">
<h2>Available Randomization Tests<a class="headerlink" href="#available-randomization-tests" title="Link to this heading"></a></h2>
<p>We offer the possibility to test how much the performance of your model deteriorates when you randomize the input training data.
We have several randomization modes and types available.</p>
<p>The modes are supplied via <code class="docutils literal notranslate"><span class="pre">--randomization_mode</span></code> and the types via <code class="docutils literal notranslate"><span class="pre">--randomization_type</span></code>.:</p>
<ul class="simple">
<li><p><strong>SVCC: Single View Constant for Cell Lines:</strong> A single cell line view (e.g., gene expression) is held unperturbed
while the others are randomized.</p></li>
<li><p><strong>SVCD: Single View Constant for Drugs:</strong> A single drug view (e.g., drug fingerprints) is held unperturbed while the
others are randomized.</p></li>
<li><p><strong>SVRC: Single View Random for Cell Lines:</strong> A single cell line view (e.g., gene expression) is randomized while the
others are held unperturbed.</p></li>
<li><p><strong>SVRD: Single View Random for Drugs:</strong> A single drug view (e.g., drug fingerprints) is randomized while the others
are held unperturbed.</p></li>
</ul>
<p>Currently, we support two ways of randomizing the data. The default is permututation.</p>
<ul class="simple">
<li><p><strong>Permutation</strong>: Permutes the features over the instances, keeping the distribution of the features the same but
dissolving the relationship to the target.</p></li>
<li><p><strong>Invariant</strong>: The randomization is done in a way that a key characteristic of the feature is preserved. In case
of matrices, this is the mean and standard deviation of the feature view for this instance, for networks it is the
degree distribution.</p></li>
</ul>
</section>
<section id="robustness-test">
<h2>Robustness Test<a class="headerlink" href="#robustness-test" title="Link to this heading"></a></h2>
<p>The robustness test is a test where the model is trained with varying seeds. This is done multiple times to see how
stable the model is. Via <code class="docutils literal notranslate"><span class="pre">--n_trials_robustness</span></code>, you can specify the number of trials for the robustness tests.</p>
</section>
<section id="available-metrics">
<h2>Available Metrics<a class="headerlink" href="#available-metrics" title="Link to this heading"></a></h2>
<p>We offer a variety of metrics to evaluate your model on. The default is the R^2 score. You can change the metric via
the <code class="docutils literal notranslate"><span class="pre">--optim_metric</span></code> parameter. The following metrics are available:</p>
<ul class="simple">
<li><p><strong>R^2</strong>: The coefficient of determination. The higher the better.</p></li>
<li><p><strong>MSE</strong>: The mean squared error. The lower the better.</p></li>
<li><p><strong>RMSE</strong>: The root mean squared error. The lower the better.</p></li>
<li><p><strong>MAE</strong>: The mean absolute error. The lower the better.</p></li>
<li><p><strong>Pearson</strong>: The Pearson correlation coefficient. The higher the better.</p></li>
<li><p><strong>Spearman</strong>: The Spearman correlation coefficient. The higher the better.</p></li>
<li><p><strong>Kendall</strong>: The Kendall correlation coefficient. The higher the better.</p></li>
<li><p><strong>Normalized [R^2, Pearson, Spearman, Kendall]</strong>: A version of the metric where the true and predicted response values are normalized by the predictions of the NaiveMeanEffectsPredictor.</p></li>
</ul>
</section>
<section id="available-response-transformations">
<h2>Available Response Transformations<a class="headerlink" href="#available-response-transformations" title="Link to this heading"></a></h2>
<p>We offer the possibility to transform the response data before training the model. This can be done via the
<code class="docutils literal notranslate"><span class="pre">--response_transformation</span></code> parameter. The following transformations are available:</p>
<ul class="simple">
<li><p><strong>None</strong>: No transformation is applied.</p></li>
<li><p><strong>standard</strong>: The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">sklearn StandardScaler</a> is applied.</p></li>
<li><p><strong>minmax</strong>: The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html">sklearn MinMaxScaler</a> is applied.</p></li>
<li><p><strong>robust</strong>: The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html">sklearn RobustScaler</a> is applied.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runyourmodel.html" class="btn btn-neutral float-right" title="Run your own model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, DaiSyBio at Technical University of Munich.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>